// This is an autogenerated file from Firebase Studio.
'use server';

/**
 * @fileOverview Evaluates user responses in vocabulary, grammar, reading, and writing modules.
 *
 * - evaluateUserResponse - A function that handles the evaluation of user responses using AI.
 * - EvaluateUserResponseInput - The input type for the evaluateUserResponse function.
 * - EvaluateUserResponseOutput - The return type for the evaluateUserResponse function.
 */

import {ai} from '@/ai/genkit';
import {z} from 'genkit';

const EvaluateUserResponseInputSchema = z.object({
  moduleType: z.enum(['vocabulary', 'grammar', 'reading', 'writing', 'wordTest']).describe('The type of module the user is responding to.'),
  userResponse: z.string().describe('The user\u2019s response to the question or task.'),
  expectedAnswer: z.string().optional().describe('The expected answer to the question or task, if applicable.'),
  questionContext: z.string().describe('The context of the question or task.'),
  userLevel: z.enum(['A0', 'A1', 'A2', 'B1', 'B2', 'C1', 'C2']).describe('The user\u2019s proficiency level in German.'),
  grammarRules: z.string().optional().describe('Relevant grammar rules for the given user level.'),
});

export type EvaluateUserResponseInput = z.infer<typeof EvaluateUserResponseInputSchema>;

const EvaluateUserResponseOutputSchema = z.object({
  evaluation: z.string().describe('An evaluation of the user\u2019s response, including feedback and suggestions for improvement.'),
  isCorrect: z.boolean().describe('Whether the user response is correct or not.'),
  suggestedCorrection: z.string().optional().describe('A suggested correction to the user\u2019s response, if applicable.'),
});

export type EvaluateUserResponseOutput = z.infer<typeof EvaluateUserResponseOutputSchema>;

export async function evaluateUserResponse(input: EvaluateUserResponseInput): Promise<EvaluateUserResponseOutput> {
  return evaluateUserResponseFlow(input);
}

const evaluateUserResponsePrompt = ai.definePrompt({
  name: 'evaluateUserResponsePrompt',
  input: {schema: EvaluateUserResponseInputSchema},
  output: {schema: EvaluateUserResponseOutputSchema},
  prompt: `You are an AI-powered German language tutor. Your task is to evaluate a user's response to a question or task.

Here is the context of the question or task: {{{questionContext}}}
Here is the user's response: {{{userResponse}}}
Here is the user's current proficiency level in German: {{{userLevel}}}

{{#if expectedAnswer}}Here is the expected answer: {{{expectedAnswer}}}{{/if}}
{{#if grammarRules}}Here are some grammar rules relevant to the user level: {{{grammarRules}}}{{/if}}

Provide an evaluation of the user's response. Include feedback and suggestions for improvement, adjusted to the user's proficiency level. Indicate whether the response is correct or not. If the response is incorrect, provide a suggested correction.

{{#if (eq moduleType "wordTest")}}
This is a 'wordTest' module. The user is being tested on their knowledge of vocabulary.
- Be stricter in your evaluation.
- If the user's response is incorrect, clearly state the correct answer in the 'suggestedCorrection'.
- The primary goal is to assess if the user knows the word.
{{else if (eq moduleType "vocabulary")}}
This is a 'vocabulary' learning module. The user is learning new words.
- Be encouraging.
- If the user's response is incorrect, provide the 'suggestedCorrection' and explain briefly if necessary.
{{else if (eq moduleType "grammar")}}
This is a 'grammar' module.
- Focus on grammatical correctness according to the user's level and the provided grammar rules.
{{else if (eq moduleType "reading")}}
This is a 'reading' module.
- Evaluate comprehension of the provided text based on the user's answer to the question.
{{else if (eq moduleType "writing")}}
This is a 'writing' module.
- Evaluate the user's written text for clarity, grammar, and relevance to the prompt.
{{/if}}

IMPORTANT: If you provide a 'suggestedCorrection', ensure it is appropriate for the user's level ({{{userLevel}}}). If a more complex correction would be ideal but is above the user's level, first provide the ideal correction, and then explicitly state: "This might be a bit advanced. A simpler way to say this at your level ({{{userLevel}}}) would be: [simpler correction]". If the ideal correction is already appropriate for the user's level, you don't need to add the "simpler way" part.

Focus on these aspects for evaluation, depending on the module type:
1.  Correctness of the answer.
2.  Semantic relevance to the topic.
3.  Grammatical accuracy according to CEFR level {{{userLevel}}}.
4.  Use of key vocabulary from the topic (if applicable for modules other than vocabulary/wordTest).

Ensure your response is in Russian.

Output your response in the following JSON format:
{
  "evaluation": "Evaluation of the user's response...",
  "isCorrect": true or false,
  "suggestedCorrection": "Suggested correction, if applicable..."
}`,
});

const evaluateUserResponseFlow = ai.defineFlow(
  {
    name: 'evaluateUserResponseFlow',
    inputSchema: EvaluateUserResponseInputSchema,
    outputSchema: EvaluateUserResponseOutputSchema,
  },
  async input => {
    const {output} = await evaluateUserResponsePrompt(input);
    return output!;
  }
);