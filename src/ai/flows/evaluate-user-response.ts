// This is an autogenerated file from Firebase Studio.
'use server';

/**
 * @fileOverview Evaluates user responses in vocabulary, grammar, reading, and writing modules.
 *
 * - evaluateUserResponse - A function that handles the evaluation of user responses using AI.
 * - EvaluateUserResponseInput - The input type for the evaluateUserResponse function.
 * - EvaluateUserResponseOutput - The return type for the evaluateUserResponse function.
 */

import {ai} from '@/ai/genkit';
import {z} from 'genkit';

const EvaluateUserResponseInputSchema = z.object({
  moduleType: z.enum(['vocabulary', 'grammar', 'reading', 'writing']).describe('The type of module the user is responding to.'),
  userResponse: z.string().describe('The user\u2019s response to the question or task.'),
  expectedAnswer: z.string().optional().describe('The expected answer to the question or task, if applicable.'),
  questionContext: z.string().describe('The context of the question or task.'),
  userLevel: z.enum(['A0', 'A1', 'A2', 'B1', 'B2', 'C1', 'C2']).describe('The user\u2019s proficiency level in German.'),
  grammarRules: z.string().optional().describe('Relevant grammar rules for the given user level.'),
});

export type EvaluateUserResponseInput = z.infer<typeof EvaluateUserResponseInputSchema>;

const EvaluateUserResponseOutputSchema = z.object({
  evaluation: z.string().describe('An evaluation of the user\u2019s response, including feedback and suggestions for improvement.'),
  isCorrect: z.boolean().describe('Whether the user response is correct or not.'),
  suggestedCorrection: z.string().optional().describe('A suggested correction to the user\u2019s response, if applicable.'),
});

export type EvaluateUserResponseOutput = z.infer<typeof EvaluateUserResponseOutputSchema>;

export async function evaluateUserResponse(input: EvaluateUserResponseInput): Promise<EvaluateUserResponseOutput> {
  return evaluateUserResponseFlow(input);
}

const evaluateUserResponsePrompt = ai.definePrompt({
  name: 'evaluateUserResponsePrompt',
  input: {schema: EvaluateUserResponseInputSchema},
  output: {schema: EvaluateUserResponseOutputSchema},
  prompt: `You are an AI-powered German language tutor. Your task is to evaluate a user's response to a question or task in a specific module (vocabulary, grammar, reading, or writing).

Here is the context of the question or task: {{{questionContext}}}

Here is the user's response: {{{userResponse}}}

Here is the user's current proficiency level in German: {{{userLevel}}}

{% if expectedAnswer %}Here is the expected answer: {{{expectedAnswer}}}{% endif %}

{% if grammarRules %}Here are some grammar rules relevant to the user level: {{{grammarRules}}}{% endif %}

Provide an evaluation of the user's response. Include feedback and suggestions for improvement, adjusted to the user's proficiency level. Indicate whether the response is correct or not. If the response is incorrect, provide a suggested correction.

Ensure your response is in Russian.

Output your response in the following JSON format:
{
  "evaluation": "Evaluation of the user's response...",
  "isCorrect": true or false,
  "suggestedCorrection": "Suggested correction, if applicable..."
}`,
});

const evaluateUserResponseFlow = ai.defineFlow(
  {
    name: 'evaluateUserResponseFlow',
    inputSchema: EvaluateUserResponseInputSchema,
    outputSchema: EvaluateUserResponseOutputSchema,
  },
  async input => {
    const {output} = await evaluateUserResponsePrompt(input);
    return output!;
  }
);
