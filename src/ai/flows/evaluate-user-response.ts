
// This is an autogenerated file from Firebase Studio.
'use server';

/**
 * @fileOverview Evaluates user responses in vocabulary, grammar, reading, and writing modules.
 *
 * - evaluateUserResponse - A function that handles the evaluation of user responses using AI.
 * - EvaluateUserResponseInput - The input type for the evaluateUserResponse function.
 * - EvaluateUserResponseOutput - The return type for the evaluateUserResponse function.
 */

import {ai} from '@/ai/genkit';
import {z} from 'genkit';
import type { WritingEvaluationDetails, ErrorExplanation } from '@/types/german-learning';


const EvaluateUserResponseInputSchema = z.object({
  moduleType: z.enum(['vocabulary', 'grammar', 'reading', 'writing', 'wordTest', 'listening']).describe('The type of module the user is responding to.'),
  userResponse: z.string().describe('The user’s response to the question or task.'),
  expectedAnswer: z.string().optional().describe('The expected answer to the question or task, if applicable.'),
  questionContext: z.string().describe('The context of the question or task.'),
  userLevel: z.enum(['A0', 'A1', 'A2', 'B1', 'B2', 'C1', 'C2']).describe('The user’s proficiency level in German.'),
  grammarRules: z.string().optional().describe('Relevant grammar rules for the given user level.'),
  // Boolean flags for Handlebars templating based on moduleType
  isModuleWordTest: z.boolean().optional().describe('Internal flag: true if moduleType is "wordTest".'),
  isModuleVocabulary: z.boolean().optional().describe('Internal flag: true if moduleType is "vocabulary".'),
  isModuleGrammar: z.boolean().optional().describe('Internal flag: true if moduleType is "grammar".'),
  isModuleReading: z.boolean().optional().describe('Internal flag: true if moduleType is "reading".'),
  isModuleWriting: z.boolean().optional().describe('Internal flag: true if moduleType is "writing".'),
  isModuleListening: z.boolean().optional().describe('Internal flag: true if moduleType is "listening".'),
});

export type EvaluateUserResponseInput = z.infer<typeof EvaluateUserResponseInputSchema>;

const WritingEvaluationDetailsSchema = z.object({
  taskAchievement: z.string().optional().describe("Feedback on how well the user addressed the writing prompt. (Оценка выполнения задания. ДОЛЖНО БЫТЬ НА РУССКОМ, ОЧЕНЬ КРАТКО)"),
  coherenceAndCohesion: z.string().optional().describe("Feedback on the logical structure and flow of the text. (Оценка связности и логичности текста. ДОЛЖНО БЫТЬ НА РУССКОМ, КРАТКО)"),
  lexicalResource: z.string().optional().describe("Feedback on the appropriateness and richness of vocabulary. (Оценка использования лексики. ДОЛЖНО БЫТЬ НА РУССКОМ, КРАТКО)"),
  grammaticalAccuracy: z.string().optional().describe("Feedback on grammatical correctness. (Оценка грамматической правильности. ДОЛЖНО БЫТЬ НА РУССКОМ, КРАТКО)"),
  overallFeedback: z.string().describe("A summary of the overall performance and key suggestions. (Общее заключение и ключевые рекомендации. ДОЛЖНО БЫТЬ НА РУССКОМ, 1-2 ПРЕДЛОЖЕНИЯ)"),
  suggestedImprovements: z.array(z.string()).optional().describe("Specific sentences or phrases that could be improved, with suggestions. (Конкретные предложения по улучшению. НА РУССКОМ)")
}).describe("Detailed feedback specific to the writing module, if applicable. This object itself is optional.");

const ErrorExplanationSchema = z.object({
    generalExplanation: z.string().describe("Общее объяснение типа ошибки на русском языке. Должно быть четким и по существу."),
    specificExample: z.string().optional().describe("Конкретный пример из ответа пользователя, ВЫДЕЛЯЮЩИЙ ошибку (если применимо и если ошибка не во всем ответе)."),
    correctionExample: z.string().optional().describe("Пример исправления для ОШИБОЧНОГО ФРАГМЕНТА ответа пользователя (если применимо), а не полный правильный ответ. Полный ответ указывается в поле 'suggestedCorrection'."),
    theoryReference: z.string().optional().describe("Краткое упоминание соответствующего грамматического правила или лексического нюанса на русском языке (если применимо, например, 'Предлог 'mit' требует Dativ').")
}).describe("Детальное объяснение ошибки, если ответ неверный (для модулей НЕ 'writing'). Это поле опционально. ВСЯ ИНФОРМАЦИЯ ВНУТРИ ЭТОГО ОБЪЕКТА ДОЛЖНА БЫТЬ НА РУССКОМ ЯЗЫКЕ.");


const EvaluateUserResponseOutputSchema = z.object({
  evaluation: z.string().describe('Краткий общий вывод на русском языке. Для письма это будет writingDetails.overallFeedback. Для других модулей – основная мысль оценки (например, "Верно!" или "Не совсем точно, есть ошибка в употреблении слова").'),
  isCorrect: z.boolean().describe('Whether the user response is correct or not. For writing, true if the text is generally understandable and addresses the prompt adequately for the level.'),
  suggestedCorrection: z.string().optional().describe('ПОЛНЫЙ предлагаемый правильный ответ на русском языке, если ответ пользователя был коротким и неверным, или ключевые исправления. Для письма это может быть исправленная версия текста.'),
  grammarErrorTags: z.array(z.string()).optional().describe('Specific grammar points the user made errors on (e.g., "akkusativ_prepositions", "verb_conjugation_modal"). Provide these only if the response is incorrect and the error is related to a specific, identifiable grammar rule or pattern relevant to the user\'s level. Use short, snake_case, English tags.'),
  writingDetails: WritingEvaluationDetailsSchema.optional(),
  errorExplanationDetails: ErrorExplanationSchema.optional()
});

export type EvaluateUserResponseOutput = z.infer<typeof EvaluateUserResponseOutputSchema> & {
  writingDetails?: WritingEvaluationDetails;
  errorExplanationDetails?: ErrorExplanation;
};

export async function evaluateUserResponse(input: EvaluateUserResponseInput): Promise<EvaluateUserResponseOutput> {
  const promptInputData = {
    ...input,
    isModuleWordTest: input.moduleType === 'wordTest',
    isModuleVocabulary: input.moduleType === 'vocabulary',
    isModuleGrammar: input.moduleType === 'grammar',
    isModuleReading: input.moduleType === 'reading',
    isModuleWriting: input.moduleType === 'writing',
    isModuleListening: input.moduleType === 'listening',
  };
  return evaluateUserResponseFlow(promptInputData);
}

const evaluateUserResponsePrompt = ai.definePrompt({
  name: 'evaluateUserResponsePrompt',
  input: {schema: EvaluateUserResponseInputSchema},
  output: {schema: EvaluateUserResponseOutputSchema},
  prompt: `You are an AI-powered German language tutor. Your task is to evaluate a user's response to a question or task.

Here is the context of the question or task: {{{questionContext}}}
Here is the user's response: {{{userResponse}}}
Here is the user's current proficiency level in German: {{{userLevel}}}

{{#if expectedAnswer}}Here is the expected answer: {{{expectedAnswer}}}{{/if}}
{{#if grammarRules}}Here are some grammar rules relevant to the user level: {{{grammarRules}}}{{/if}}

Provide an evaluation of the user's response. Include feedback and suggestions for improvement, adjusted to the user's proficiency level. Indicate whether the response is correct or not.
The main 'evaluation' field MUST be a concise summary in RUSSIAN.
If the response is incorrect, provide a 'suggestedCorrection' (full corrected answer, in RUSSIAN) if appropriate.

If the module type is 'grammar' or 'writing', and the user's response is incorrect due to specific grammatical errors relevant to their level ({{{userLevel}}}), please identify these errors and list them in the 'grammarErrorTags' field. Use short, English, snake_case tags (e.g., "nominative_case_error", "verb_position_subclause", "modal_verb_usage", "adjective_declension_dativ", "perfekt_auxiliary_verb"). Only include tags for clear, identifiable grammatical mistakes, not stylistic issues or minor vocabulary errors.

{{#unless isModuleWriting}}
  {{#unless isCorrect}}
    If the answer is incorrect (for modules other than 'writing'), provide detailed feedback in the 'errorExplanationDetails' object. This feedback MUST be in RUSSIAN and concise. Do not be overly verbose. Focus on the main error and its explanation.
    - 'generalExplanation': Clearly explain in RUSSIAN why the user's answer is incorrect. Be specific to the error type (e.g., wrong vocabulary choice, specific grammatical mistake like case or tense, misunderstanding of the question). Keep it brief (1-2 sentences).
    - 'specificExample': If applicable and the error is localized, quote the specific incorrect part of the user's answer.
    - 'correctionExample': If you provided a 'specificExample', show how *that specific part* should be corrected in RUSSIAN. This is for a snippet, not the full corrected answer. The main 'suggestedCorrection' field can be used for the full corrected answer.
    - 'theoryReference': If the error relates to a specific grammar rule (e.g., "Предлог 'mit' всегда требует дательного падежа (Dativ)") or a vocabulary nuance (e.g., "Слово 'aktuell' означает 'актуальный, текущий', а не 'фактический'"), briefly mention this rule/nuance in RUSSIAN.
  {{/unless}}
{{/unless}}


{{#if isModuleWordTest}}
This is a 'wordTest' module. The user is being tested on their knowledge of vocabulary.
- Be stricter in your evaluation. The main 'evaluation' should clearly state if it's correct or not.
- If the user's response is incorrect, clearly state the correct answer in the 'suggestedCorrection' (in RUSSIAN).
- The primary goal is to assess if the user knows the word.
- For 'errorExplanationDetails' (if incorrect):
    - 'generalExplanation' should explain why the given translation is wrong.
    - 'theoryReference' could point out gender/plural if that was part of the expected knowledge or a common confusion.
{{/if}}
{{#if isModuleVocabulary}}
This is a 'vocabulary' learning module. The user is learning new words.
- Be encouraging in the 'evaluation' field.
- If the user's response is incorrect, provide the 'suggestedCorrection' (in RUSSIAN) and explain briefly in 'errorExplanationDetails.generalExplanation'.
- 'errorExplanationDetails.theoryReference' can mention gender/plural or usage notes.
{{/if}}
{{#if isModuleGrammar}}
This is a 'grammar' module.
- Focus on grammatical correctness according to the user's level and the provided grammar rules.
- If incorrect, provide 'grammarErrorTags' if applicable.
- 'errorExplanationDetails' should focus on the specific grammar mistake, referencing the rule in 'theoryReference'.
{{/if}}
{{#if isModuleListening}}
This is a 'listening' module.
- Evaluate comprehension of the provided audio script based on the user's answer to the question. The main 'evaluation' should reflect this.
- If the user's answer is incorrect, 'errorExplanationDetails' should clarify the misunderstanding of the audio content.
- Provide 'grammarErrorTags' only if the user's answer *itself* contains significant grammatical errors that hinder understanding, and these errors are relevant to their learning level.
{{/if}}
{{#if isModuleReading}}
This is a 'reading' module.
- Evaluate comprehension of the provided text based on the user's answer to the question. The main 'evaluation' should reflect this.
- If the user's answer is incorrect, 'errorExplanationDetails' should clarify the misunderstanding of the text.
- Provide 'grammarErrorTags' only if the user's answer *itself* contains significant grammatical errors that hinder understanding, and these errors are relevant to their learning level.
{{/if}}
{{#if isModuleWriting}}
This is a 'writing' module. For the writing module, ALL feedback in 'writingDetails' (including 'taskAchievement', 'coherenceAndCohesion', 'lexicalResource', 'grammaticalAccuracy', 'overallFeedback', and 'suggestedImprovements') MUST be concise and in RUSSIAN. Avoid lengthy elaborations or general commentary outside of \`overallFeedback\`.
- When providing feedback in the 'writingDetails' object, ensure each field ('taskAchievement', 'coherenceAndCohesion', etc.) contains specific, DISTINCT, and CONCISE information relevant ONLY to that criterion. ALL TEXT MUST BE IN RUSSIAN.
- Evaluate the user's written text based on the following criteria, appropriate for their level ({{{userLevel}}}):
  1.  'taskAchievement' (Оценка выполнения задания): This field MUST be in RUSSIAN and EXTREMELY CONCISE.
      **CRITICAL INSTRUCTIONS FOR 'taskAchievement':**
      a.  Your response for 'taskAchievement' MUST NOT exceed 30 words and should ideally be 1-2 short, factual sentences.
      b.  **DO NOT re-list or rephrase the requirements that were given in the original writing prompt ({{{questionContext}}}).**
      c.  Instead, **FACTUALLY STATE** whether the user's text *fulfilled* those specific requirements from {{{questionContext}}}.
      d.  **Examples based on prompt ({{{questionContext}}}):**
          *   If prompt ({{{questionContext}}}) was "Напишите 3-5 предложений о ваших выходных, используя слова X, Y, Z":
              *   A GOOD 'taskAchievement': "Написано 4 предложения. Тема выходных раскрыта. Использованы слова X и Y; слово Z не использовано." (Factual, checks specific requirements)
              *   A BAD 'taskAchievement' (DO NOT DO THIS): "Пользователь должен был написать 3-5 предложений. Пользователь должен был использовать слова X, Y, Z. Пользователь написал о выходных." (This is re-listing, not assessing)
          *   If prompt ({{{questionContext}}}) was "Напишите короткое электронное письмо другу о планах на отпуск.":
              *   A GOOD 'taskAchievement': "Написано электронное письмо другу. Тема планов на отпуск раскрыта. Формат письма соблюден."
              *   A BAD 'taskAchievement': "Пользователю нужно было написать email. Тема - отпуск. Пользователь написал email."
      e.  Focus ONLY on whether the user *did what was explicitly asked* in {{{questionContext}}} (e.g., length, specific words if requested, topic, format).
      f.  DO NOT evaluate language quality (grammar, style) in this field. Use other fields for that.
      g.  DO NOT add praise, general comments, or motivational phrases here.
      h.  **Each part of your statement for 'taskAchievement' must convey unique factual information about the fulfillment of a *specific requirement* from the original prompt. Do NOT repeat the same assessment or point multiple times, even with different wording.**
  2.  'coherenceAndCohesion' (Оценка связности и логичности текста): This field MUST be in RUSSIAN. Evaluate ONLY the text's logical structure, flow, and use of linking words. No praise or generic comments. Keep this section concise (1-2 sentences).
  3.  'lexicalResource' (Оценка использования лексики): This field MUST be in RUSSIAN. Evaluate ONLY vocabulary choice, range, and appropriateness for the level and topic. No praise or generic comments. Keep this section concise (1-2 sentences).
  4.  'grammaticalAccuracy' (Оценка грамматической правильности): This field MUST be in RUSSIAN. Evaluate ONLY grammatical correctness and range of structures used, appropriate for the level. No praise or generic comments. Keep this section concise (1-2 sentences).
- Provide detailed feedback in the 'writingDetails' object with these fields. ALL TEXT IN 'writingDetails' MUST BE IN RUSSIAN.
- The 'writingDetails.overallFeedback' field (Общее заключение и ключевые рекомендации) MUST be in RUSSIAN and contain a VERY BRIEF (1-2 sentences maximum) holistic summary of overall performance. If you wish to add a short motivational sentence, this is the *only* place for it, and it must be unique and not repeated. THIS SUMMARY SHOULD BE THE VALUE for the main 'evaluation' field of the entire response.
- The 'writingDetails.suggestedImprovements' field (Конкретные предложения по улучшению) MUST be in RUSSIAN and can contain 1-3 *specific* examples from the user's text with suggested rewrites, if applicable. Focus on concrete examples.
- CRITICAL: Do NOT use lengthy, repetitive motivational phrases such as 'Вместе мы - непобедимы! У вас все получится! Я всегда буду рядом...' or similar extended encouragements in any field of 'writingDetails'. If any encouragement is given in \`overallFeedback\`, it must be extremely brief (a few words) and not repeated. The user needs specific, actionable feedback, not repeated motivational slogans.
- Set 'isCorrect' to true if the text is generally understandable, adequately addresses the prompt for the user's level ({{{userLevel}}}), and doesn't contain an overwhelming number of errors that impede communication. Minor errors are acceptable for a 'true' evaluation, especially at lower levels.
- If 'isCorrect' is false due to grammar, provide relevant 'grammarErrorTags'.
- The 'suggestedCorrection' field can optionally contain a fully corrected version of the user's text (in RUSSIAN) if it's short and the corrections are significant, or a few key corrections otherwise.
{{/if}}

IMPORTANT: If you provide a 'suggestedCorrection', ensure it is appropriate for the user's level ({{{userLevel}}}). If a more complex correction would be ideal but is above the user's level, first provide the ideal correction, and then explicitly state: "Это может быть немного продвинуто. Более простой способ сказать это на вашем уровне ({{{userLevel}}}): [simpler correction]". If the ideal correction is already appropriate for the user's level, you don't need to add the "более простой способ" часть. ALL TEXT IN THIS EXPLANATION MUST BE IN RUSSIAN.

Focus on these aspects for evaluation, depending on the module type:
1.  Correctness of the answer.
2.  Semantic relevance to the topic.
3.  Grammatical accuracy according to CEFR level {{{userLevel}}}.
4.  Use of key vocabulary from the topic (if applicable for modules other than vocabulary/wordTest).

Ensure ALL your string output values in the JSON object are in RUSSIAN, especially within 'writingDetails' and 'errorExplanationDetails'.

Output your response in the following JSON format (ensure grammarErrorTags is an array of strings, or omit if not applicable; ensure writingDetails and errorExplanationDetails are objects or omit if not applicable):
{
  "evaluation": "Ваш краткий общий вывод на русском...",
  "isCorrect": true,
  "suggestedCorrection": "Полный исправленный ответ на русском, если нужно...",
  "grammarErrorTags": [],
  "writingDetails": {
    "taskAchievement": "Задание выполнено: объем соответствует, тема раскрыта.",
    "coherenceAndCohesion": "...",
    "lexicalResource": "...",
    "grammaticalAccuracy": "...",
    "overallFeedback": "...",
    "suggestedImprovements": []
  },
  "errorExplanationDetails": {
    "generalExplanation": "Общее объяснение ошибки на русском...",
    "specificExample": "Пример ошибки из ответа пользователя...",
    "correctionExample": "Исправление этого примера...",
    "theoryReference": "Ссылка на теорию на русском..."
  }
}`,
});

const MAX_RETRIES = 5;
const INITIAL_RETRY_DELAY_MS = 3000;

const evaluateUserResponseFlow = ai.defineFlow(
  {
    name: 'evaluateUserResponseFlow',
    inputSchema: EvaluateUserResponseInputSchema,
    outputSchema: EvaluateUserResponseOutputSchema,
  },
  async (inputWithFlags: EvaluateUserResponseInput) => {
    let retries = 0;
    while (retries < MAX_RETRIES) {
      try {
        const {output} = await evaluateUserResponsePrompt(inputWithFlags);
        if (!output) {
          throw new Error('[evaluateUserResponseFlow] AI model returned an empty output during response evaluation.');
        }
        // Ensure that if it's a writing module, the 'evaluation' field is populated from 'writingDetails.overallFeedback'
        if (inputWithFlags.isModuleWriting && output.writingDetails && output.writingDetails.overallFeedback && !output.evaluation) {
            output.evaluation = output.writingDetails.overallFeedback;
        } else if (inputWithFlags.isModuleWriting && output.writingDetails && output.writingDetails.overallFeedback && output.evaluation !== output.writingDetails.overallFeedback) {
            // If AI provided both, prefer the specific overallFeedback from writingDetails for the main evaluation field.
            output.evaluation = output.writingDetails.overallFeedback;
        }
        return output;
      } catch (error: any) {
        retries++;
        const inputKeys = inputWithFlags ? Object.keys(inputWithFlags).join(', ') : 'undefined input';
        console.error(`[evaluateUserResponseFlow] Attempt ${retries} FAILED. Input keys: ${inputKeys}. Error:`, error.message ? error.message : error);
        if (retries >= MAX_RETRIES) {
          console.error(`[evaluateUserResponseFlow] All ${MAX_RETRIES} retries FAILED for input:`, JSON.stringify(inputWithFlags, null, 2), "Last error:", error.message ? error.message : error);
          throw error;
        }

        const errorMessage = error.message ? error.message.toLowerCase() : '';
        if (
          errorMessage.includes('503') ||
          errorMessage.includes('service unavailable') ||
          errorMessage.includes('model is overloaded') ||
          errorMessage.includes('server error') ||
          errorMessage.includes('internal error')
        ) {
          const delay = INITIAL_RETRY_DELAY_MS * Math.pow(2, retries - 1);
          console.warn(`[evaluateUserResponseFlow] Attempt ${retries} failed with transient error. Retrying in ${delay / 1000}s...`);
          await new Promise(resolve => setTimeout(resolve, delay));
        } else {
          console.error('[evaluateUserResponseFlow] Failed with non-retryable error. Input:', JSON.stringify(inputWithFlags, null, 2), 'Error:', error.message ? error.message : error);
          throw error;
        }
      }
    }
    throw new Error('[evaluateUserResponseFlow] Failed after multiple retries, and loop exited unexpectedly.');
  }
);

