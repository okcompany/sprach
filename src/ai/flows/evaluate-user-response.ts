
// This is an autogenerated file from Firebase Studio.
'use server';

/**
 * @fileOverview Evaluates user responses in vocabulary, grammar, reading, and writing modules.
 *
 * - evaluateUserResponse - A function that handles the evaluation of user responses using AI.
 * - EvaluateUserResponseInput - The input type for the evaluateUserResponse function.
 * - EvaluateUserResponseOutput - The return type for the evaluateUserResponse function.
 */

import {ai} from '@/ai/genkit';
import {z} from 'genkit';
import type { WritingEvaluationDetails } from '@/types/german-learning';


const EvaluateUserResponseInputSchema = z.object({
  moduleType: z.enum(['vocabulary', 'grammar', 'reading', 'writing', 'wordTest', 'listening']).describe('The type of module the user is responding to.'),
  userResponse: z.string().describe('The user’s response to the question or task.'),
  expectedAnswer: z.string().optional().describe('The expected answer to the question or task, if applicable.'),
  questionContext: z.string().describe('The context of the question or task.'),
  userLevel: z.enum(['A0', 'A1', 'A2', 'B1', 'B2', 'C1', 'C2']).describe('The user’s proficiency level in German.'),
  grammarRules: z.string().optional().describe('Relevant grammar rules for the given user level.'),
  // Boolean flags for Handlebars templating based on moduleType
  isModuleWordTest: z.boolean().optional().describe('Internal flag: true if moduleType is "wordTest".'),
  isModuleVocabulary: z.boolean().optional().describe('Internal flag: true if moduleType is "vocabulary".'),
  isModuleGrammar: z.boolean().optional().describe('Internal flag: true if moduleType is "grammar".'),
  isModuleReading: z.boolean().optional().describe('Internal flag: true if moduleType is "reading".'),
  isModuleWriting: z.boolean().optional().describe('Internal flag: true if moduleType is "writing".'),
  isModuleListening: z.boolean().optional().describe('Internal flag: true if moduleType is "listening".'),
});

export type EvaluateUserResponseInput = z.infer<typeof EvaluateUserResponseInputSchema>;

const WritingEvaluationDetailsSchema = z.object({
  taskAchievement: z.string().optional().describe("Feedback on how well the user addressed the writing prompt. (Оценка выполнения задания. ДОЛЖНО БЫТЬ НА РУССКОМ, ОЧЕНЬ КРАТКО)"),
  coherenceAndCohesion: z.string().optional().describe("Feedback on the logical structure and flow of the text. (Оценка связности и логичности текста. ДОЛЖНО БЫТЬ НА РУССКОМ, КРАТКО)"),
  lexicalResource: z.string().optional().describe("Feedback on the appropriateness and richness of vocabulary. (Оценка использования лексики. ДОЛЖНО БЫТЬ НА РУССКОМ, КРАТКО)"),
  grammaticalAccuracy: z.string().optional().describe("Feedback on grammatical correctness. (Оценка грамматической правильности. ДОЛЖНО БЫТЬ НА РУССКОМ, КРАТКО)"),
  overallFeedback: z.string().describe("A summary of the overall performance and key suggestions. (Общее заключение и ключевые рекомендации. ДОЛЖНО БЫТЬ НА РУССКОМ, 1-2 ПРЕДЛОЖЕНИЯ)"),
  suggestedImprovements: z.array(z.string()).optional().describe("Specific sentences or phrases that could be improved, with suggestions. (Конкретные предложения по улучшению. НА РУССКОМ)")
}).describe("Detailed feedback specific to the writing module, if applicable. This object itself is optional.");


const EvaluateUserResponseOutputSchema = z.object({
  evaluation: z.string().describe('An evaluation of the user’s response, including feedback and suggestions for improvement. For writing, this will be the overallFeedback from writingDetails.'),
  isCorrect: z.boolean().describe('Whether the user response is correct or not. For writing, true if the text is generally understandable and addresses the prompt adequately for the level.'),
  suggestedCorrection: z.string().optional().describe('A suggested correction to the user’s response, if applicable. For writing, this could be a fully corrected version of the text if feasible, or key corrections.'),
  grammarErrorTags: z.array(z.string()).optional().describe('Specific grammar points the user made errors on (e.g., "akkusativ_prepositions", "verb_conjugation_modal"). Provide these only if the response is incorrect and the error is related to a specific, identifiable grammar rule or pattern relevant to the user\'s level. Use short, snake_case, English tags.'),
  writingDetails: WritingEvaluationDetailsSchema.optional()
});

export type EvaluateUserResponseOutput = z.infer<typeof EvaluateUserResponseOutputSchema> & {
  writingDetails?: WritingEvaluationDetails; // Ensure this is part of the TS type
};

export async function evaluateUserResponse(input: EvaluateUserResponseInput): Promise<EvaluateUserResponseOutput> {
  // Populate boolean flags for templating based on input.moduleType
  const promptInputData = {
    ...input,
    isModuleWordTest: input.moduleType === 'wordTest',
    isModuleVocabulary: input.moduleType === 'vocabulary',
    isModuleGrammar: input.moduleType === 'grammar',
    isModuleReading: input.moduleType === 'reading',
    isModuleWriting: input.moduleType === 'writing',
    isModuleListening: input.moduleType === 'listening',
  };
  return evaluateUserResponseFlow(promptInputData);
}

const evaluateUserResponsePrompt = ai.definePrompt({
  name: 'evaluateUserResponsePrompt',
  input: {schema: EvaluateUserResponseInputSchema},
  output: {schema: EvaluateUserResponseOutputSchema},
  prompt: `You are an AI-powered German language tutor. Your task is to evaluate a user's response to a question or task.

Here is the context of the question or task: {{{questionContext}}}
Here is the user's response: {{{userResponse}}}
Here is the user's current proficiency level in German: {{{userLevel}}}

{{#if expectedAnswer}}Here is the expected answer: {{{expectedAnswer}}}{{/if}}
{{#if grammarRules}}Here are some grammar rules relevant to the user level: {{{grammarRules}}}{{/if}}

Provide an evaluation of the user's response. Include feedback and suggestions for improvement, adjusted to the user's proficiency level. Indicate whether the response is correct or not. If the response is incorrect, provide a suggested correction.

If the module type is 'grammar' or 'writing', and the user's response is incorrect due to specific grammatical errors relevant to their level ({{{userLevel}}}), please identify these errors and list them in the 'grammarErrorTags' field. Use short, English, snake_case tags (e.g., "nominative_case_error", "verb_position_subclause", "modal_verb_usage", "adjective_declension_dativ", "perfekt_auxiliary_verb"). Only include tags for clear, identifiable grammatical mistakes, not stylistic issues or minor vocabulary errors.

{{#if isModuleWordTest}}
This is a 'wordTest' module. The user is being tested on their knowledge of vocabulary.
- Be stricter in your evaluation.
- If the user's response is incorrect, clearly state the correct answer in the 'suggestedCorrection'.
- The primary goal is to assess if the user knows the word.
- Do not provide 'grammarErrorTags' for this module type unless the error is directly tied to a grammatical aspect of a single word (e.g. gender of a noun if tested).
{{/if}}
{{#if isModuleVocabulary}}
This is a 'vocabulary' learning module. The user is learning new words.
- Be encouraging.
- If the user's response is incorrect, provide the 'suggestedCorrection' and explain briefly if necessary.
- Do not provide 'grammarErrorTags' for this module type.
{{/if}}
{{#if isModuleGrammar}}
This is a 'grammar' module.
- Focus on grammatical correctness according to the user's level and the provided grammar rules.
- If incorrect, provide 'grammarErrorTags' if applicable.
{{/if}}
{{#if isModuleListening}}
This is a 'listening' module.
- Evaluate comprehension of the provided audio script based on the user's answer to the question.
- Provide 'grammarErrorTags' only if the user's answer itself contains significant grammatical errors that hinder understanding, and these errors are relevant to their learning level.
{{/if}}
{{#if isModuleReading}}
This is a 'reading' module.
- Evaluate comprehension of the provided text based on the user's answer to the question.
- Provide 'grammarErrorTags' only if the user's answer itself contains significant grammatical errors that hinder understanding, and these errors are relevant to their learning level.
{{/if}}
{{#if isModuleWriting}}
This is a 'writing' module. For the writing module, ALL feedback in 'writingDetails' (including 'taskAchievement', 'coherenceAndCohesion', 'lexicalResource', 'grammaticalAccuracy', 'overallFeedback', and 'suggestedImprovements') MUST be concise and in RUSSIAN. Avoid lengthy elaborations or general commentary outside of \`overallFeedback\`.
- When providing feedback in the 'writingDetails' object, ensure each field ('taskAchievement', 'coherenceAndCohesion', etc.) contains specific, DISTINCT, and CONCISE information relevant ONLY to that criterion. ALL TEXT MUST BE IN RUSSIAN.
- Evaluate the user's written text based on the following criteria, appropriate for their level ({{{userLevel}}}):
  1.  'taskAchievement' (Оценка выполнения задания): This field MUST be in RUSSIAN. Strictly evaluate ONLY if the task was completed as per instructions (e.g., did they write about the topic, include specific information if asked, was it the right length for the level?). Provide a 1-2 sentence factual statement MAXIMUM. Do NOT include general praise, summaries, motivational phrases, or any text in English. Example of a good, concise, Russian response for this field: "Задание выполнено: пользователь рассказал о себе, упомянув имя, возраст и место жительства. Объем соответствует уровню." Avoid any further elaboration or repetition.
  2.  'coherenceAndCohesion' (Оценка связности и логичности текста): This field MUST be in RUSSIAN. Evaluate ONLY the text's logical structure, flow, and use of linking words. No praise or generic comments. Keep this section concise (1-2 sentences).
  3.  'lexicalResource' (Оценка использования лексики): This field MUST be in RUSSIAN. Evaluate ONLY vocabulary choice, range, and appropriateness for the level and topic. No praise or generic comments. Keep this section concise (1-2 sentences).
  4.  'grammaticalAccuracy' (Оценка грамматической правильности): This field MUST be in RUSSIAN. Evaluate ONLY grammatical correctness and range of structures used, appropriate for the level. No praise or generic comments. Keep this section concise (1-2 sentences).
- Provide detailed feedback in the 'writingDetails' object with these fields. ALL TEXT IN 'writingDetails' MUST BE IN RUSSIAN.
- The 'writingDetails.overallFeedback' field (Общее заключение и ключевые рекомендации) MUST be in RUSSIAN and contain a VERY BRIEF (1-2 sentences maximum) holistic summary of overall performance. If you wish to add a short motivational sentence, this is the *only* place for it, and it must be unique and not repeated. THIS SUMMARY SHOULD BE THE VALUE for the main 'evaluation' field of the entire response.
- The 'writingDetails.suggestedImprovements' field (Конкретные предложения по улучшению) MUST be in RUSSIAN and can contain 1-3 *specific* examples from the user's text with suggested rewrites, if applicable. Focus on concrete examples.
- CRITICAL: Do NOT use lengthy, repetitive motivational phrases such as 'Вместе мы - непобедимы! У вас все получится! Я всегда буду рядом...' or similar extended encouragements in any field of 'writingDetails'. If any encouragement is given in \`overallFeedback\`, it must be extremely brief (a few words) and not repeated. The user needs specific, actionable feedback, not repeated motivational slogans.
- Set 'isCorrect' to true if the text is generally understandable, adequately addresses the prompt for the user's level ({{{userLevel}}}), and doesn't contain an overwhelming number of errors that impede communication. Minor errors are acceptable for a 'true' evaluation, especially at lower levels.
- If 'isCorrect' is false due to grammar, provide relevant 'grammarErrorTags'.
- The 'suggestedCorrection' field can optionally contain a fully corrected version of the user's text if it's short and the corrections are significant, or a few key corrections otherwise. This field, if present, MUST also be in RUSSIAN.
{{/if}}

IMPORTANT: If you provide a 'suggestedCorrection', ensure it is appropriate for the user's level ({{{userLevel}}}). If a more complex correction would be ideal but is above the user's level, first provide the ideal correction, and then explicitly state: "This might be a bit advanced. A simpler way to say this at your level ({{{userLevel}}}) would be: [simpler correction]". If the ideal correction is already appropriate for the user's level, you don't need to add the "simpler way" part. ALL TEXT IN THIS EXPLANATION MUST BE IN RUSSIAN.

Focus on these aspects for evaluation, depending on the module type:
1.  Correctness of the answer.
2.  Semantic relevance to the topic.
3.  Grammatical accuracy according to CEFR level {{{userLevel}}}.
4.  Use of key vocabulary from the topic (if applicable for modules other than vocabulary/wordTest).

Ensure your response is in Russian. All string values in the output JSON object, especially within 'writingDetails', MUST be in RUSSIAN.

Output your response in the following JSON format (ensure grammarErrorTags is an array of strings, or omit if not applicable; ensure writingDetails is an object or omit if not applicable):
{
  "evaluation": "Evaluation of the user's response...",
  "isCorrect": true or false,
  "suggestedCorrection": "Suggested correction, if applicable...",
  "grammarErrorTags": ["tag1", "tag2"],
  "writingDetails": {
    "taskAchievement": "...",
    "coherenceAndCohesion": "...",
    "lexicalResource": "...",
    "grammaticalAccuracy": "...",
    "overallFeedback": "...",
    "suggestedImprovements": ["..."]
  }
}`,
});

const MAX_RETRIES = 5;
const INITIAL_RETRY_DELAY_MS = 3000;

const evaluateUserResponseFlow = ai.defineFlow(
  {
    name: 'evaluateUserResponseFlow',
    inputSchema: EvaluateUserResponseInputSchema,
    outputSchema: EvaluateUserResponseOutputSchema,
  },
  async (inputWithFlags: EvaluateUserResponseInput) => {
    let retries = 0;
    while (retries < MAX_RETRIES) {
      try {
        const {output} = await evaluateUserResponsePrompt(inputWithFlags);
        if (!output) {
          throw new Error('[evaluateUserResponseFlow] AI model returned an empty output during response evaluation.');
        }
        // Ensure that if it's a writing module, the 'evaluation' field is populated from 'writingDetails.overallFeedback'
        if (inputWithFlags.isModuleWriting && output.writingDetails && output.writingDetails.overallFeedback && !output.evaluation) {
            output.evaluation = output.writingDetails.overallFeedback;
        } else if (inputWithFlags.isModuleWriting && output.writingDetails && output.writingDetails.overallFeedback && output.evaluation !== output.writingDetails.overallFeedback) {
            // If AI provided both, prefer the specific overallFeedback from writingDetails for the main evaluation field.
            output.evaluation = output.writingDetails.overallFeedback;
        }
        return output;
      } catch (error: any) {
        retries++;
        const inputKeys = inputWithFlags ? Object.keys(inputWithFlags).join(', ') : 'undefined input';
        console.error(`[evaluateUserResponseFlow] Attempt ${retries} FAILED. Input keys: ${inputKeys}. Error:`, error.message ? error.message : error);
        if (retries >= MAX_RETRIES) {
          console.error(`[evaluateUserResponseFlow] All ${MAX_RETRIES} retries FAILED for input:`, JSON.stringify(inputWithFlags, null, 2), "Last error:", error.message ? error.message : error);
          throw error;
        }

        const errorMessage = error.message ? error.message.toLowerCase() : '';
        if (
          errorMessage.includes('503') ||
          errorMessage.includes('service unavailable') ||
          errorMessage.includes('model is overloaded') ||
          errorMessage.includes('server error') ||
          errorMessage.includes('internal error')
        ) {
          const delay = INITIAL_RETRY_DELAY_MS * Math.pow(2, retries - 1);
          console.warn(`[evaluateUserResponseFlow] Attempt ${retries} failed with transient error. Retrying in ${delay / 1000}s...`);
          await new Promise(resolve => setTimeout(resolve, delay));
        } else {
          console.error('[evaluateUserResponseFlow] Failed with non-retryable error. Input:', JSON.stringify(inputWithFlags, null, 2), 'Error:', error.message ? error.message : error);
          throw error;
        }
      }
    }
    throw new Error('[evaluateUserResponseFlow] Failed after multiple retries, and loop exited unexpectedly.');
  }
);

