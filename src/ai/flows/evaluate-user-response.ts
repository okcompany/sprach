
// This is an autogenerated file from Firebase Studio.
'use server';

/**
 * @fileOverview Evaluates user responses in vocabulary, grammar, reading, and writing modules.
 *
 * - evaluateUserResponse - A function that handles the evaluation of user responses using AI.
 * - EvaluateUserResponseInput - The input type for the evaluateUserResponse function.
 * - EvaluateUserResponseOutput - The return type for the evaluateUserResponse function.
 */

import {ai} from '@/ai/genkit';
import {z} from 'genkit';

const EvaluateUserResponseInputSchema = z.object({
  moduleType: z.enum(['vocabulary', 'grammar', 'reading', 'writing', 'wordTest', 'listening']).describe('The type of module the user is responding to.'),
  userResponse: z.string().describe('The user\u2019s response to the question or task.'),
  expectedAnswer: z.string().optional().describe('The expected answer to the question or task, if applicable.'),
  questionContext: z.string().describe('The context of the question or task.'),
  userLevel: z.enum(['A0', 'A1', 'A2', 'B1', 'B2', 'C1', 'C2']).describe('The user\u2019s proficiency level in German.'),
  grammarRules: z.string().optional().describe('Relevant grammar rules for the given user level.'),
  // Boolean flags for Handlebars templating based on moduleType
  isModuleWordTest: z.boolean().optional().describe('Internal flag: true if moduleType is "wordTest".'),
  isModuleVocabulary: z.boolean().optional().describe('Internal flag: true if moduleType is "vocabulary".'),
  isModuleGrammar: z.boolean().optional().describe('Internal flag: true if moduleType is "grammar".'),
  isModuleReading: z.boolean().optional().describe('Internal flag: true if moduleType is "reading".'),
  isModuleWriting: z.boolean().optional().describe('Internal flag: true if moduleType is "writing".'),
  isModuleListening: z.boolean().optional().describe('Internal flag: true if moduleType is "listening".'),
});

export type EvaluateUserResponseInput = z.infer<typeof EvaluateUserResponseInputSchema>;

const EvaluateUserResponseOutputSchema = z.object({
  evaluation: z.string().describe('An evaluation of the user\u2019s response, including feedback and suggestions for improvement.'),
  isCorrect: z.boolean().describe('Whether the user response is correct or not.'),
  suggestedCorrection: z.string().optional().describe('A suggested correction to the user\u2019s response, if applicable.'),
  grammarErrorTags: z.array(z.string()).optional().describe('Specific grammar points the user made errors on (e.g., "akkusativ_prepositions", "verb_conjugation_modal"). Provide these only if the response is incorrect and the error is related to a specific, identifiable grammar rule or pattern relevant to the user\'s level. Use short, snake_case, English tags.'),
});

export type EvaluateUserResponseOutput = z.infer<typeof EvaluateUserResponseOutputSchema>;

export async function evaluateUserResponse(input: EvaluateUserResponseInput): Promise<EvaluateUserResponseOutput> {
  // Populate boolean flags for templating based on input.moduleType
  const promptInputData = {
    ...input,
    isModuleWordTest: input.moduleType === 'wordTest',
    isModuleVocabulary: input.moduleType === 'vocabulary',
    isModuleGrammar: input.moduleType === 'grammar',
    isModuleReading: input.moduleType === 'reading',
    isModuleWriting: input.moduleType === 'writing',
    isModuleListening: input.moduleType === 'listening',
  };
  return evaluateUserResponseFlow(promptInputData);
}

const evaluateUserResponsePrompt = ai.definePrompt({
  name: 'evaluateUserResponsePrompt',
  input: {schema: EvaluateUserResponseInputSchema}, 
  output: {schema: EvaluateUserResponseOutputSchema},
  prompt: `You are an AI-powered German language tutor. Your task is to evaluate a user's response to a question or task.

Here is the context of the question or task: {{{questionContext}}}
Here is the user's response: {{{userResponse}}}
Here is the user's current proficiency level in German: {{{userLevel}}}

{{#if expectedAnswer}}Here is the expected answer: {{{expectedAnswer}}}{{/if}}
{{#if grammarRules}}Here are some grammar rules relevant to the user level: {{{grammarRules}}}{{/if}}

Provide an evaluation of the user's response. Include feedback and suggestions for improvement, adjusted to the user's proficiency level. Indicate whether the response is correct or not. If the response is incorrect, provide a suggested correction.

If the module type is 'grammar' or 'writing', and the user's response is incorrect due to specific grammatical errors relevant to their level ({{{userLevel}}}), please identify these errors and list them in the 'grammarErrorTags' field. Use short, English, snake_case tags (e.g., "nominative_case_error", "verb_position_subclause", "modal_verb_usage", "adjective_declension_dativ", "perfekt_auxiliary_verb"). Only include tags for clear, identifiable grammatical mistakes, not stylistic issues or minor vocabulary errors.

{{#if isModuleWordTest}}
This is a 'wordTest' module. The user is being tested on their knowledge of vocabulary.
- Be stricter in your evaluation.
- If the user's response is incorrect, clearly state the correct answer in the 'suggestedCorrection'.
- The primary goal is to assess if the user knows the word.
- Do not provide 'grammarErrorTags' for this module type unless the error is directly tied to a grammatical aspect of a single word (e.g. gender of a noun if tested).
{{/if}}
{{#if isModuleVocabulary}}
This is a 'vocabulary' learning module. The user is learning new words.
- Be encouraging.
- If the user's response is incorrect, provide the 'suggestedCorrection' and explain briefly if necessary.
- Do not provide 'grammarErrorTags' for this module type.
{{/if}}
{{#if isModuleGrammar}}
This is a 'grammar' module.
- Focus on grammatical correctness according to the user's level and the provided grammar rules.
- If incorrect, provide 'grammarErrorTags' if applicable.
{{/if}}
{{#if isModuleListening}}
This is a 'listening' module.
- Evaluate comprehension of the provided audio script based on the user's answer to the question.
- Provide 'grammarErrorTags' only if the user's answer itself contains significant grammatical errors that hinder understanding, and these errors are relevant to their learning level.
{{/if}}
{{#if isModuleReading}}
This is a 'reading' module.
- Evaluate comprehension of the provided text based on the user's answer to the question.
- Provide 'grammarErrorTags' only if the user's answer itself contains significant grammatical errors that hinder understanding, and these errors are relevant to their learning level.
{{/if}}
{{#if isModuleWriting}}
This is a 'writing' module.
- Evaluate the user's written text for clarity, grammar, and relevance to the prompt.
- If incorrect due to grammar, provide 'grammarErrorTags'.
{{/if}}

IMPORTANT: If you provide a 'suggestedCorrection', ensure it is appropriate for the user's level ({{{userLevel}}}). If a more complex correction would be ideal but is above the user's level, first provide the ideal correction, and then explicitly state: "This might be a bit advanced. A simpler way to say this at your level ({{{userLevel}}}) would be: [simpler correction]". If the ideal correction is already appropriate for the user's level, you don't need to add the "simpler way" part.

Focus on these aspects for evaluation, depending on the module type:
1.  Correctness of the answer.
2.  Semantic relevance to the topic.
3.  Grammatical accuracy according to CEFR level {{{userLevel}}}.
4.  Use of key vocabulary from the topic (if applicable for modules other than vocabulary/wordTest).

Ensure your response is in Russian.

Output your response in the following JSON format (ensure grammarErrorTags is an array of strings, or omit if not applicable):
{
  "evaluation": "Evaluation of the user's response...",
  "isCorrect": true or false,
  "suggestedCorrection": "Suggested correction, if applicable...",
  "grammarErrorTags": ["tag1", "tag2"]
}`,
});

const MAX_RETRIES = 5;
const INITIAL_RETRY_DELAY_MS = 3000;

const evaluateUserResponseFlow = ai.defineFlow(
  {
    name: 'evaluateUserResponseFlow',
    inputSchema: EvaluateUserResponseInputSchema, 
    outputSchema: EvaluateUserResponseOutputSchema,
  },
  async (inputWithFlags: EvaluateUserResponseInput) => { 
    let retries = 0;
    while (retries < MAX_RETRIES) {
      try {
        const {output} = await evaluateUserResponsePrompt(inputWithFlags); 
        if (!output) {
          throw new Error('[evaluateUserResponseFlow] AI model returned an empty output during response evaluation.');
        }
        return output;
      } catch (error: any) {
        retries++;
        const inputKeys = inputWithFlags ? Object.keys(inputWithFlags).join(', ') : 'undefined input';
        console.error(`[evaluateUserResponseFlow] Attempt ${retries} FAILED. Input keys: ${inputKeys}. Error:`, error.message ? error.message : error);
        if (retries >= MAX_RETRIES) {
          console.error(`[evaluateUserResponseFlow] All ${MAX_RETRIES} retries FAILED for input:`, JSON.stringify(inputWithFlags, null, 2), "Last error:", error.message ? error.message : error);
          throw error; 
        }
        
        const errorMessage = error.message ? error.message.toLowerCase() : '';
        if (
          errorMessage.includes('503') ||
          errorMessage.includes('service unavailable') ||
          errorMessage.includes('model is overloaded') ||
          errorMessage.includes('server error') || 
          errorMessage.includes('internal error') 
        ) {
          const delay = INITIAL_RETRY_DELAY_MS * Math.pow(2, retries - 1); 
          console.warn(`[evaluateUserResponseFlow] Attempt ${retries} failed with transient error. Retrying in ${delay / 1000}s...`);
          await new Promise(resolve => setTimeout(resolve, delay));
        } else {
          console.error('[evaluateUserResponseFlow] Failed with non-retryable error. Input:', JSON.stringify(inputWithFlags, null, 2), 'Error:', error.message ? error.message : error);
          throw error;
        }
      }
    }
    throw new Error('[evaluateUserResponseFlow] Failed after multiple retries, and loop exited unexpectedly.');
  }
);

